<!--
title: Project Information - Part 2
description: 
published: true
date: 2020-12-08T12:31:59.178Z
tags: 
editor: ckeditor
dateCreated: 2020-12-08T11:21:31.762Z
-->

<h1>FlashBack&nbsp;</h1>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2>FlashBack Marketing Tasks&nbsp;</h2>
<p>&nbsp;</p>
<h3><strong>Immediate priorities:</strong></h3>
<p><br><strong>Business - Personal pricing</strong></p>
<p>The prices are decided, the pages are ready. We're waiting on changes from Oliver, a minor ones from Alex. Then we can make it live. Hopefully before the 14th. &nbsp;<br>&nbsp;</p>
<p><br>&nbsp;</p>
<p><strong>Skype inbound campaign</strong></p>
<p>We were seeing very low conversion (to download) rates from the "Record&nbsp;Skype" ads, on Facebook and Adwords. I've stopped the Facebook ads for now and put their budget into Adwords, which were converting better.&nbsp;We've changed the landing page and made the ad keywords more targeted and this&nbsp;&nbsp;has improved the rate, but not by a huge amount. We're surprised by this - the ads are targetted and the page relevant.&nbsp;Matt is making one more revision of the page, and I'll work on the keywords again when we have more data.</p>
<p>&nbsp;</p>
<p><strong>Next inbound campaign</strong></p>
<p>Matt has suggested "Flipped classroom". We can reuse text already written by Anna for&nbsp;the landing page, which Matt will start on very soon. Facebook ads may get better returns this time - we should be able to target educators &nbsp;using their system, which is not possible using adwords.</p>
<p>&nbsp;</p>
<p><strong>Application improvements</strong></p>
<p>-&nbsp;FB UI</p>
<p>The&nbsp;main change we're working on is to give a consistent 'home' screen for FB. When FB starts, we show a 'Welcome' screen with links to tutorials, recent movies and to 'Record the screen'. But the user never returns to this. Other screen recorders have a similar screen that the user is returned to after closing the Recorder. Were still&nbsp;working on a new design that displays Help, Recordings and feels like a genuine improvement over the current one. Other changes to the region and window recording UI, 'inspired' by other recorders, are already designed.&nbsp;</p>
<p>&nbsp;<br>&nbsp;-&nbsp;Tutorials</p>
<p>As part of the UI changes, we plan to make a few&nbsp;single page tutorials on basic tasks. A page, in Hubspot will be far easier to maintain than a movie.</p>
<p>-&nbsp;Improved upsell from Express</p>
<p>This is a&nbsp;top priority for October.&nbsp;Currently we make no effort to upsell Express users on Plus/Pro, so even if we&nbsp;only succeed with&nbsp;a very small fraction it could&nbsp;represent a significant rise in&nbsp;sales. A first attempt at&nbsp;implementing this was unsuccessful, but the second try is looking good. We should be there before the 22nd.</p>
<p>&nbsp;&nbsp;<br>&nbsp;</p>
<p><br><strong>Email campaigns</strong></p>
<p>When the Business-person pricing is in place, we'll change the Pro/Plus email&nbsp;campaigns to send a discount on day 3. Mark has suggested Pro Business -&gt; $69 and Personal -&gt; $29. Ok.</p>
<p><br>We will also start to email Express users, pushing Pro/Plus (no discount).&nbsp;Currently we don't push Express downloaders into HS, but it should be simple for&nbsp;Oliver to do this. We&nbsp;can filter out &gt;90% of them in HS and there should still be enough to get data on email clicks, and whether visitors from the email buy.</p>
<p><strong>More inbound marketing</strong></p>
<p>Matt is keen to get a blog running (in Hubspot), using an agency to write articles based on briefs from ourselves. The content can be reused and linked from social media.&nbsp;We have 4 themes we'd like to start with and Matt is getting prices.</p>
<p><strong>Remarketing ads</strong></p>
<p>We've stalled on this a bit. We had images, but they needed improving. I'd looked into running a remarketing campaign in adwords.&nbsp;Once we have a set of images,&nbsp;then we should be able to get some ads going soon.</p>
<p>&nbsp;</p>
<p><strong>FBX</strong></p>
<p>John seems to have cracked the crash problem. Victor is working on one more set of improvements, and then its out to BB staff. I've been working on UI, don't have anything I'm entirely happy with yet, but we need to move on, so it may be a 'v1' for the beta.&nbsp;Matt has got a download/landing&nbsp;page designed. I have a design doc outlining things like account creation, UI,&nbsp;how we implement beta and trials, what websites we need,&nbsp;that John has been reading and commenting on. There's a lot for him to do and&nbsp;I hope he can get good focus on this for the rest of the year.</p>
<h3>For Later:</h3>
<p><strong>Affiliates</strong></p>
<p>I know some people are cold on affiliates, but we're keen to see if we can test the water because its another option&nbsp;we've never&nbsp;tried.</p>
<p>&nbsp;</p>
<p>We're faced with the choice of either going with an existing affiliate network or making our own. Using an existing network should get us more affiliates, quickly, but will take a %. We've already done most of the work for tracking sales for resellers, so much of the work in creating our own network is in the sales reporting, settlements. This could be a fair sized task though. The approach we've suggested before is to try a network, to see if we get anything. If we do, we can consider making our network. If not, leave it.&nbsp;</p>
<p>&nbsp;</p>
<p>We've also wondered about an 'affiliate-lite' kind of approach, targetting Youtube users. We have a "Make money from your Youtube videos" campaign, with banners on the site, emails, social media. They sign up, get a unique URL to post in the description of their video. When someone downloads and purchases off that URL, they get a payment. We give them a basic page to track downloads, sales. By not saying "this is an affiliate system", we're wondering if we can reduce the requirements, but settlements to the affiliates may be difficult.</p>
<p><br>&nbsp;</p>
<p><strong>Store</strong></p>
<p>Less urgent since we've improved the exit rate from the final step of the purchase process, but would still like to try a single page purchase process for FB, using the new iframe-less Braintree API.<br>&nbsp;</p>
<p><strong>TA</strong></p>
<p>Reduce to one edition. Reduce price in line with FB. Bring it into the main site.</p>
<p><br>&nbsp;</p>
<p><strong>FB Trial period</strong></p>
<p>Trials are generally 30 days, but Movavi go for a shorter 7 days. Pros for a shorter period: you can send that 'last chance discount' mail sooner, and you may reduce the number of people that get what they want from FB just via the trial (we've increased the watermark size recently though). Cons: some users don't get the chance to use FB within 7 days and will need to ask for extension of their trial licence.&nbsp;</p>
<p><br>&nbsp;</p>
<p><br>&nbsp;</p>
<p><strong>Licence-less&nbsp;</strong><br>&nbsp;</p>
<p>We use a system that sends a licence to the user to confirm their email address. After one day of trial (or immediately, with Express) the user needs to enter the licence key we emailed them the day before, to use FB again. None of our competitors that I'm aware of use a system like this. Camtasia collects an email address for marketing, but they don't send a licence to it. About 2/3 of Pro licences and 3/4 of Plus licences we send out are never entered into FB, so that's 2/3 and 3/4 of installations that cannot be used beyond the first day. This looks like something we can improve on. At the very least, we could give users a longer 'licence-less' period. What we're trying to improve here is engagement with FB, so the stats we can watch are those on the number of times recorder and player are run, and movies recorded.&nbsp;<br><br>&nbsp;</p>
<p><strong>Localisation</strong></p>
<p>To prove or disprove the "sell cheap in populous low-income countries" theory, we</p>
<p>&nbsp; - Translate the website or main pages into Russian. How that's best implemented is to be figured out. I'm hoping we can do something using simple subfolders in HS, so we don't need to purchase a translation management system.<br>&nbsp;</p>
<p>&nbsp; - Mark wants to set prices for FB-Russian lower? Ok.</p>
<p>&nbsp; - We could add a lower priority task for Oliver to look at adding <a href="https://money">https://money</a>.ru or <a href="https://www.webmoney.ru/">https://www.webmoney.ru</a></p>
<p>&nbsp;</p>
<h1>Traka&nbsp;</h1>
<h2>Installer&nbsp;</h2>
<p>&nbsp;</p>
<h2>Task:</h2>
<p>Make an installer grouping 4msi together using the technology called WIX.</p>
<h2>Requirement:</h2>
<p>Wix (toolset) v3.10</p>
<h2>location:</h2>
<p><a href="https://svn.bbconsult.co.uk/repository/trakaplc_trakinst/trunk/Wix">https://svn.bbconsult.co.uk/repository/trakaplc_trakinst/trunk/Wix</a></p>
<p>&nbsp;</p>
<h3>About WIX&nbsp;</h3>
<p>&nbsp;</p>
<p>Wix is a tool set that allows you to create windows installers using XML format.</p>
<p>It has mainly two set of tools:</p>
<ol>
  <li>The standard, which allows the creation of a normal installer.</li>
  <li>And Burn or also named Wix Bootstrapper which allows bundling multiple installers together and chain install them.</li>
</ol>
<p>The standard has some descent tutorials and instruction to follow which can be found in the external resources of the Wix Documentation page: <a href="http://wixtoolset.org/documentation/">http://wixtoolset.org/documentation/</a></p>
<p>The bootstrapper can be a bit more tricky as not a lot of tutorial have been made and most of the instruction are going to be pointing to Wix actual installer's source code.</p>
<h3>Wix Bootstrapper (Wix Burn)</h3>
<p>&nbsp;</p>
<p>The first thing to know about Wix's Bootstrapper is that it's separated in three parts:</p>
<ol>
  <li>The bundle</li>
  <li>The Bootstrapper / UI in c#</li>
  <li>The Bootstrapper engine</li>
</ol>
<p>The bundle is where you will declare all your msi, the conditions regarding the machine and the <strong>bootstrapper to use</strong>.</p>
<p>Wix does provides some basic bootstrappers that can be used but they are really light and aren't much flexible.</p>
<p>If some more complex behavior are needed instead of what the basic bootstrappers Wix provides you will need to make a custom Bootstrapper.</p>
<p>All you need to do to create a&nbsp;bootstrapper is to have a class inherit from the abstract class "BootstrapperApplication" and you got your bootstrapper which also contains its engine. The BootstrapperApplication class is located in "Microsoft.Tools.WindowsInstallerXml.Bootstrapper" in the&nbsp;Microsoft.Deployment.WindowsInstaller.dll which is provided by Wix's SDK.&nbsp;</p>
<p>Your bootstrapper will have a range of event handler which can be categorized in three installation phases: Detection, Planning and Applying.</p>
<ul>
  <li>The Detection phase is where the bootstrapper will look for the status of the future msi to install, if there are absent, present or need an update. This is where you will be able to populate your installer's UI with the correct values.</li>
  <li>The Panning phase is where the bootstrapper will cash the msi and make a plan of what needs to be installed, uninstalled or repaired. This is where you will be able to choose if the installer will have to skip a certain msi based on some logic in your UI.</li>
  <li>The Applying phase is where the bootstrapper does his magic and go thought the planning to execute the msi.</li>
</ul>
<p>Each of those need to be started by the engine by calling it's designated method ex: Bootstrapper.Engine.Detect().</p>
<p>The whole process of the bootstrapper really rely on calling those methods and handling the events which aren't well documented.</p>
<p>Some documentation can be found in the source code under src\burn\inc\IBootstrapperApplication.h which can be useful to know what return value you can use but most of the time you would just want to log the information that get passed though and work from there.</p>
<p>Finally here is a useful link to the most complete tutorial I found: <a href="http://www.wrightfully.com/part-1-of-writing-your-own-net-based-installer-with-wix-overview">http://www.wrightfully.com/part-1-of-writing-your-own-net-based-installer-with-wix-overview</a></p>
<p>&nbsp;</p>
<h1>Parcel SafePlace&nbsp;</h1>
<h2>Parcel SafePlace (PackageRoom) iPad application&nbsp;</h2>
<p>&nbsp;</p>
<p>This document aims to describe the build and release process for the Parcel SafePlace iPad application.</p>
<h2>Source code</h2>
<p>The source code for the application resides in the <a href="https://svn.bbconsult.co.uk/repository/packageroom/trunk/iPad%20application/">packageroom</a> repository. Use SVN to checkout the code to somewhere on your Mac that Xcode can see.</p>
<p>A VM has already been set up within the Birmingham office with the necessary tools and configuration to release the application. See the BitWarden note 'macOS VM: Parcel Safeplace'.</p>
<p>&nbsp;</p>
<h2>Build environment</h2>
<p>The iPad application requires the following to build:</p>
<ul>
  <li>Xcode</li>
  <li>Developer certificates, either personal or from the <strong>Parcel Safe Place Limited</strong> account</li>
</ul>
<p>The iPad application requires the following to release:</p>
<ul>
  <li>Xcode Organiser / Application Loader</li>
  <li>Being part of the Parcel Safe Place Limited enterprise team (ask Ben Waters at Parcel Safeplace for help with this)</li>
  <li>Possessing the Parcel Safeplace distribution certificate</li>
</ul>
<h2>Preparing the environment</h2>
<p>Open&nbsp;<strong>Xcode&nbsp;→ Preferences</strong> and add the account you used to join the Parcel Safeplace Limited enterprise team. In our case, this is <strong>apple@bbconsult.co.uk</strong>, the details of which are in the 'BB Consult Dev Account' BW note.&nbsp;Once signed in, click <strong>View Details...</strong> when highlighting the&nbsp;<strong>Parcel Safe Place Limited</strong> team&nbsp;and ensure that iOS development and iOS distribution certificates are on your machine. If they aren't, use the&nbsp;<strong>Create</strong>&nbsp;button to request them. Download any provisioning profiles using the&nbsp;<strong>Download all&nbsp;</strong>button.</p>
<p>If the distribution certificate needs renewing or resetting, follow the instructions at <a href="https://developer.apple.com">developer.apple.com</a> in the&nbsp;<strong>Certificates, Identifiers &amp; Profiles</strong>&nbsp;page, otherwise skip to the next section. Click&nbsp;<strong>Certificates&nbsp;→ Production&nbsp;</strong>and follow the instructions. You'll need to generate a certificate signing request using the&nbsp;<strong>Keychain Access</strong> app on your Mac. Click&nbsp;<strong>Keychain Access&nbsp;→ Certificate Assistant&nbsp;→ Request a Certificate from a Certificate Authority.</strong> Use&nbsp;<strong>admin@parcelsafeplace.com&nbsp;</strong>in the email field,&nbsp;<strong>Parcel SafePlace&nbsp;</strong>in the common name field, and leave the CA email field blank.</p>
<figure class="image"><img src="attachments/85493753/85493754.png"></figure>
<p>Click the option to save the file to disk. Now upload the CSR when requested by the certificate request wizard on the Apple developer portal, and it should return a new certificate which you can then import back into&nbsp;<strong>Keychain Access</strong> using&nbsp;<strong>File&nbsp;→ Import</strong>. You should now see the new certificate in the&nbsp;<strong>login</strong> keychain. The old distribution certificate/key can safely be removed. The instructions are the same for normal developer certificates, just choose&nbsp;<strong>Certificates&nbsp;→ Development&nbsp;</strong>in the developer portal.</p>
<p>As you've created a new distribution certificate, you need to also create a new distribution provisioning profile, or edit the existing one to point to the new certificate and then download it in Xcode again. To create a new profile, select&nbsp;<strong>Provisioning Profiles&nbsp;→ Distribution</strong> in the developer portal and click the&nbsp;<strong>+</strong> icon, select App Store distribution, ensure that the correct app ID (com.parcelsafeplace) is selected, then select the new certificate you generated earlier. To edit an existing profile, select it and click&nbsp;<strong>Edit</strong>, then select the new distribution certificate you created earlier. Either option will generate a new profile, which you can then download in the Xcode account details pane.</p>
<h2>Sharing certificates/keys across machines</h2>
<p>While provisioning profiles can be downloaded again to a new machine via Xcode, the private key tied to a certificate is stored in the keychain of the Mac which generated the initial CSR. To migrate or share the certificate, the key must be exported alongside it. Right click the certificate in&nbsp;<strong>Keychain Access</strong> and select the export option. Save it somewhere, and encrypt it with a password when prompted. Use the&nbsp;<strong>File&nbsp;→ Import</strong> option on the destination machine and use the same password to decrypt when importing. Delete exported file when finished.</p>
<h2>Building and running the application</h2>
<p>Open the project in Xcode and ensure that the&nbsp;<strong>Parcel Safe Place Limited</strong>&nbsp;team is selected.</p>
<p>Select the correct scheme using the scheme menu in Xcode.</p>
<figure class="image"><img src="attachments/85493753/85493755.png?effects=drop-shadow"></figure>
<p>Click the play icon on the control bar across the top of Xcode to run the app with the active scheme.</p>
<p>Alternatively, use the&nbsp;<strong>Product</strong> menu to build without running, or run the last built version of the app.</p>
<p>Connect an iPad and select it from the scheme menu if you want to test on a physical device. This will require you to trust your Mac on the iPad, and will trigger Xcode to generate provisioning profiles for the device.</p>
<h3>Releasing the application</h3>
<p>Parcel Safeplace used to use Testflight as a distribution mechanism, but have switched to using Miradore as an MDM solution and also use it to distribute applications.</p>
<p>Ensure that you've selected a valid scheme for release. If in doubt, select the&nbsp;<strong>Generic iOS Device</strong> scheme.</p>
<p>Once you're ready to release, increment the build number in the <strong>General</strong> tab, and&nbsp;use the&nbsp;<strong>Product&nbsp;→ Archive</strong> menu to prepare a release.</p>
<p>In the organizer window, select the newly built archive and select&nbsp;<strong>Distribute App</strong>. Choose manual signing, and select 'Ad Hoc' distribution, and select the appropriate distribution certificate when prompted.</p>
<p>Alternatively, you can use the&nbsp;<strong>Application Loader </strong>in the&nbsp;<strong>Window</strong> menu of Xcode. This is not covered in this article.</p>
<p>The signed IPA is then sent to our contact at Parcel Safeplace via email. At the time of writing, this is Ben Waters.</p>
<h3>Adding devices to the provisioning profile</h3>
<p>Occasionally, Parcel Safeplace may want to add new devices to the provisioning profile used for distribution. Simply add the devices in the Apple developer portal, regenerate the profile, and then import it into Xcode.</p>
<h2>&nbsp;</h2>
<p>&nbsp;</p>
<h2>Parcel SafePlace System Architecture&nbsp;</h2>
<p>Created by Jason Cozza on Nov 24, 2020</p>
<p>Parcel SafePlace (hereafter referred to as PSP) is a system for dropping off and retrieving packages at a shared residency.</p>
<p>The system consists of three major components:</p>
<ul>
  <li>iPad application</li>
  <li>Web Portal + Database</li>
  <li>Raspberry Pi</li>
</ul>
<h2>iPad application</h2>
<p>The iPad application allows carriers to make deliveries to a residency, managers to open lockers, and residents to open their locker.</p>
<figure class="image"><img src="attachments/85493756/85493761.png?effects=drop-shadow"></figure>
<p>The app allows the carrier to take a picture of the label, which is then associated with the delivery on the web portal. For every package delivered, the carrier can select the locker to store the package in. In Dustin's case, each residency uses a shared 'Package Room', access to which is controlled by a relay connected to the Raspberry Pi. The iPad app sends a HTTP request to a web server running on the Pi, which then fires the relay via GPIO. Once a delivery has been confirmed, the web portal sends out an email and SMS to the resident with their pickup code, which they can then use to retrieve the package at their convenience.</p>
<p>If a manager enters their code they can view the status of, and override the locks, on lockers.</p>
<figure class="image"><img src="attachments/85493756/85493760.png?effects=drop-shadow"></figure>
<p>&nbsp;</p>
<h2>Web portal</h2>
<p>The web portal has multiple interfaces depending on the role of the logged in user.</p>
<h3>Administrator interface</h3>
<p>The admin interface provides a global view into the system, allowing the admin to view or modify any aspect of the system.</p>
<figure class="image"><img src="attachments/85493756/85493759.png"></figure>
<h3>Manager interface</h3>
<p>The manager interface provides a location-specific view for deliveries and residents. The location can be changed using a drop-down menu at the top-right of the page.</p>
<figure class="image"><img src="attachments/85493756/85493758.png"></figure>
<h3>Resident interface</h3>
<p>The resident interface provides a way for residents to view delivered parcels and update their personal information.</p>
<figure class="image"><img src="attachments/85493756/85493757.png"></figure>
<h3>Raspberry Pi</h3>
<p>The Pi serves the role of the hardware bridge to the lockers, allowing the iPad application or web portal to open physical lockers on site. In order to open the lockers, the Pi is equipped with a shield which is then wired to the locker(s).</p>
<p>The iPad app communicates with the Pi through a small web server on the Pi which exposes an API, allowing lockers to be opened.</p>
<p>The Pi also acts as the internet gateway for the iPad, allowing it to sync data with the web portal server.</p>
<p>Additionally, an OpenVPN client is installed which connects back to the web portal server, allowing remote diagnostics if required.</p>
<p><br>&nbsp;</p>
<h1>Keltbray&nbsp;</h1>
<p>&nbsp;</p>
<p>We have a <a href="https://keltbray-wiki.bbconsult.co.uk/">Keltbray specific Wiki</a></p>
<p>&nbsp;</p>
<h2>Haulage&nbsp;</h2>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3>Zebra Printer&nbsp;</h3>
<p>&nbsp;</p>
<p><i>For the RAGS Zebra label printer in Systems (Stonehouse) see </i><a href="https://wiki.bbconsult.co.uk/pages/viewpage.action?pageId=79135286"><i>Systems Support - Printers</i></a><br>&nbsp;</p>
<p><i>This is out of date</i></p>
<p>The Keltbary Ticketing functionality uses a Zebra Printer Bluetooth connected to the tablet</p>
<p>There is a simulator here: <a href="http://labelary.com/viewer.html">http://labelary.com/viewer.html</a>&nbsp; It’s not perfect and the font sizing can be a bit inaccurate, but it’s a good guide. Note that we are using 3-inch continuous paper so set the dimensions to be 3 x (large enough number to contain all the printed data)</p>
<p>The current code is in BBWT.Web\app\directives\formApp\printForm.ts.</p>
<p>The conveyanceModeLayout() function generates a print template for the Conveyancing form. The code here is very rough and was written as just a proof-of-concept, and I’d recommend refactoring it into a base PrintTemplateWriter class (and ConveryanceNoteTemplateWriter sub-class) before doing anything else with it, because at the moment it’s specific to the conveyance note and doesn’t expose any functionality that could be shared with other forms. The base class would provide some functionality for generic formatting (like the bold() function does), and keep track of the x and y coordinates – although the structure of the ZPL language makes it hard to map to a nice OO API.</p>
<h3>ZPL</h3>
<p>The template is in a language called ZPL, which you can find a manual on here:&nbsp; <a href="https://www.zebra.com/content/dam/zebra/manuals/en-us/software/zpl-zbi2-pm-en.pdf">https://www.zebra.com/content/dam/zebra/manuals/en-us/software/zpl-zbi2-pm-en.pdf</a>&nbsp; it’s not a nice language and it doesn’t provide you with the things you expect – for example, to do bold text you need to print the same text three times in slightly different positions. &nbsp;Something else to be aware of is that you have to set the vertical length of the printed data at the top of the form (with ^LL), but you probably won’t know this until you’ve finished generating the template.</p>
<h3>Pairing With Printer</h3>
<p>&nbsp;Also: I’ve started moving the scan() functionality into BBWT.Web\app\services\formApp\PrintService.ts (not yet committed, but will be soon). If you find that you want to add more functionality into the print directive, it might make sense to put it into the PrintService instead.</p>
<p>&nbsp;</p>
<h1>Piling&nbsp;</h1>
<p>&nbsp;</p>
<h1>MS Sync (Piling and KEF)&nbsp;</h1>
<p>&nbsp;</p>
<p>Instructions for adding tables to synchronization process.</p>
<p>&nbsp;</p>
<ol>
  <li>Your entity should be inherited from SyncEntity if instances will be created offline, otherwise from Entity. PK types other than int and Guid are not supported in Eforms (could be if needed, but it needs code changes)</li>
  <li>Create new migration and add your table to the synchronization scope like this:</li>
</ol>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;string scopeName = "designData";</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;var provisionHelper = new SyncProvisionHelper();</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;using (var serverConn = new SqlConnection(ConfigurationManager.ConnectionStrings["DefaultConnection"].ConnectionString))</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; var scopeDesc = new DbSyncScopeDescription(scopeName);</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;scopeDesc.Tables.Add(SqlSyncDescriptionBuilder.GetDescriptionForTable("BusinessArea", serverConn));</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; scopeDesc.Tables.Add(SqlSyncDescriptionBuilder.GetDescriptionForTable("BusinessAreaGroup", serverConn));</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; scopeDesc.Tables.Add(SqlSyncDescriptionBuilder.GetDescriptionForTable("FormStatus", serverConn));</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; scopeDesc.Tables.Add(SqlSyncDescriptionBuilder.GetDescriptionForTable("Ticket", serverConn));</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; scopeDesc.Tables.Add(SqlSyncDescriptionBuilder.GetDescriptionForTable("SiteUser", serverConn));</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;provisionHelper.UpdateProvision(scopeDesc, new List&lt;string&gt;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "BusinessArea",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "BusinessAreaGroup",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "FormStatus",</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Ticket"</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; });</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</p>
<p>&nbsp;</p>
<p><strong>Note for not read-only tables:</strong> Principal table should always be added before dependent table, not after. Check this for all principal tables otherwise upload of principal and dependent entity together will fail.</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;UpdateProvision() method replaces stored procedures for already added tables, so you do not need to add your tables there the first time, but need to add them in the next migrations which change synchronization.</p>
<ol>
  <li>Run this migration</li>
  <li>Open ClassesGenScript.sql in BBWT.Sync, add there your table:</li>
</ol>
<p>&nbsp;insert into @tmp (name, [readOnly]) values ('Ticket', 0);</p>
<p>&nbsp;Note: Second parameter should be 1 if the data in your table won’t be changed offline.&nbsp;</p>
<ol>
  <li>Copy this script to SQL Server Management Studio and run against your database. Then copy the result in &lt;root&gt; tag and replace the content of OfflineModels.cs (in BBWT.Sync)</li>
</ol>
<p>Note: If your entity has PK with more than one field then you need to create a separate file with model and not include your table into the script.&nbsp;</p>
<ol>
  <li>In DesignDataScope.cs add your new model collection. Name doesn’t matter.</li>
  <li>If you table needs filtering when synchronizing (like downloading only entities linked to the current user) add this logic to DataFilter.cs</li>
  <li>In app.ts (BBWT.Web) add new offline table:</li>
</ol>
<p>&nbsp;angular.extend($idbProvider.defaults, {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; version: 1,</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; name: 'Eforms',</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; onUpgradeNeeded: function (session) {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; session.createObjectStore('BusinessAreas', { keyPath: 'Id' });</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; session.createObjectStore('BusinessAreaGroups', { keyPath: 'Id' });</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; session.createObjectStore('PdfForms', { keyPath: 'Id' });</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; session.createObjectStore('TaskDefinitions', { keyPath: 'Id' });&nbsp;&nbsp;</p>
<p>Indexes if needed are also created here.&nbsp;</p>
<ol>
  <li>In SyncSvc.cs add link between server and client tables:</li>
</ol>
<p>var tablesToSync: HashTable&lt;ITable&gt; = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'BBWT.Sync.OfflineModels.BusinessArea': { localName: 'BusinessAreas', readOnly: true },</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'BBWT.Sync.OfflineModels.BusinessAreaGroup': { localName: 'BusinessAreaGroups', readOnly: true },</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'BBWT.Sync.OfflineModels.PdfForm': { localName: 'PdfForms', readOnly: true },</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'BBWT.Sync.OfflineModels.PdfFormImage': { localName: 'PdfFormImages', readOnly: true },</p>
<p>&nbsp;</p>
<p>Again if the entity has more than 1 PK you need to change getEntityId() method here.</p>
<p>&nbsp;</p>
<h1>Support&nbsp;</h1>
<p>&nbsp;</p>
<h1>About Keltbray&nbsp;</h1>
<p>&nbsp;</p>
<h2>About Keltbray</h2>
<p>Keltbray Group dates back to 1976 and is a UK leading specialist business that offers engineering, construction, demolition, decommissioning, remediation, rail, environmental services and reinforced concrete frame solutions.&nbsp; The group is a key player in developing and maintaining Britain’s built environment and operates in highly regulated environments; making sites ready for new infrastructure and developments.</p>
<p>&nbsp;</p>
<p><a href="Keltbray-Group-Structure_85493769.html">Keltbray Group Structure</a></p>
<p>&nbsp;</p>
<h1>Keltbray Group Structure&nbsp;</h1>
<p>&nbsp;</p>
<h3>Group&nbsp;Central Services</h3>
<p>Haulage</p>
<p>Plant</p>
<p>Concrete Plant</p>
<p>Training</p>
<p>klm - Occupational Health (HSQE)</p>
<p>&nbsp;</p>
<h3>Environmental</h3>
<p>Environmental</p>
<p>Remediation</p>
<p>&nbsp;</p>
<h3>Demolition &amp; Civil Engineering</h3>
<p>Demolition &amp; Civils</p>
<p>Decommissioning</p>
<p>Environmental Solutions</p>
<p>Piling</p>
<p>Sheet Piling</p>
<p>Structures</p>
<p>Wentworth House</p>
<p>Distribution &amp; Transmission</p>
<p>&nbsp;</p>
<h3>Rail</h3>
<p>Aspire</p>
<p>Electrification Plant</p>
<p>Rail</p>
<p>&nbsp;</p>
<h1>Keltbray Projects&nbsp;</h1>
<p>&nbsp;</p>
<p><a href="/pages/createpage.action?spaceKey=BLUEB&amp;title=KIS+-+Keltbray+Integrated+System">KIS - Keltbray Integrated System</a></p>
<p>&nbsp;</p>
<h1>Keltbray Support&nbsp;</h1>
<p>&nbsp;</p>
<p>Access to the Keltbray network is achieved by VPN authenticated against Keltbray's Active Directory.</p>
<p>&nbsp;</p>
<h1>PImeo - Sean Griffin&nbsp;</h1>
<p>&nbsp;</p>
<p>&nbsp;Mobile Cloud Solutions, or PImeo is a web application designed to streamline the process of construction site work, including management of employees and contractors, equipment hire, materials and costing.</p>
<p>The system is designed to allow multiple companies to manage their projects.</p>
<p>&nbsp;</p>
<h1>AWS Elastic Beanstalk deployment issue&nbsp;</h1>
<p>&nbsp;</p>
<p>EB will create a new EC2 instance if the health of the current EC2 is bad.</p>
<p>Currently if this happens the newly created EC2 will spin up without the https (port 443) bindings.&nbsp;</p>
<p>If you see the Cloudflare 522 error page then you will need to manually add the bindings to the web site in IIS</p>
<p>To do this:</p>
<ol>
  <li>Log into AWS using the PIMeo Blueberry account</li>
  <li>Find the IP address of the newly created server</li>
  <li>Retrieve the password for the server - PEM file is stored in BW</li>
  <li>RDP into the server</li>
  <li>Open IIS</li>
  <li>highlight the Default Web Site (PIMeo)</li>
  <li>Select Bindings under Edit Site on the right<br><br><br>&nbsp;</li>
</ol>
<figure class="image"><img src="attachments/85493773/85493775.png"></figure>
<ol>
  <li>Add a https binding and select the self signed certificate - as seen below</li>
</ol>
<figure class="image"><img src="attachments/85493773/85493774.png"></figure>
<p>&nbsp;</p>
<p>This should bring back pimeo.net</p>
<p>&nbsp;</p>
<p>**As of 05-10-2018**</p>
<p>Jon Allan is working with Asghar on creating a post deploy script to handle this in AWS EB.</p>
<p>&nbsp;</p>
<h1>Contacts&nbsp;</h1>
<p>&nbsp;</p>
<p>&nbsp;</p>
<figure class="table" style="width:68.719%;">
  <table>
    <tbody>
      <tr>
        <th>Name</th>
        <th colspan="1">Title</th>
        <th>Email</th>
        <th>Phone</th>
      </tr>
      <tr>
        <th colspan="1">Sean Griffin</th>
        <th colspan="1">Managing Director</th>
        <th colspan="1"><a href="mailto:sgriffin@mobilecloudsystems.com">sgriffin@mobilecloudsystems.com</a></th>
        <th colspan="1">00 353 876 773 175</th>
      </tr>
    </tbody>
  </table>
</figure>
<figure class="table" style="width:26.6304%;">
  <table>
    <tbody>
      <tr>
        <th>Team Member</th>
        <th colspan="1">Role</th>
        <th>Start (UK)</th>
        <th>Finish (UK)</th>
      </tr>
      <tr>
        <th>Eugene Lapworth</th>
        <th colspan="1">Developer</th>
        <th>9am</th>
        <th>6pm</th>
      </tr>
      <tr>
        <th>Simon Timpson</th>
        <th colspan="1">Developer</th>
        <th>7am</th>
        <th>4pm</th>
      </tr>
      <tr>
        <th>Anna Starling</th>
        <th colspan="1">Tester</th>
        <th>8am</th>
        <th>5pm</th>
      </tr>
      <tr>
        <th>Jon Fitch</th>
        <th colspan="1">Contract Developer</th>
        <th>9am</th>
        <th>5pm</th>
      </tr>
      <tr>
        <th>Jon Allan</th>
        <th colspan="1">Project Manager</th>
        <th>8am</th>
        <th>4pm</th>
      </tr>
      <tr>
        <th>Chris Murray</th>
        <th colspan="1">Assistant Project Manager</th>
        <th>9am</th>
        <th>5pm</th>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<h1>Further Issues&nbsp;</h1>
<p>&nbsp;</p>
<p>&nbsp;</p>
<figure class="table" style="width:62.663%;">
  <table>
    <tbody>
      <tr>
        <th>Description</th>
        <th>Severity</th>
        <th>Affects</th>
      </tr>
      <tr>
        <th>In sites where you can view a forms, you can changing the word 'view' to 'edit' will allow the user to edit the form, even if the option is not given to the user (CVI cannot be edited after being created, for example)</th>
        <th>High, present site-wide</th>
        <th>Security of site</th>
      </tr>
      <tr>
        <th>User passwords are sent in plain text to the server</th>
        <th>High, present site-wide</th>
        <th>Security of site</th>
      </tr>
      <tr>
        <th>User permission are not checked on WebAPI - considering Id's are used as URL parameters this leaves the site open to URL exploitation, ordering plants/materials for another organisation, accessing another organisation's data etc</th>
        <th>High, present site-wide</th>
        <th>Security of site</th>
      </tr>
      <tr>
        <th>
          <p>On <a href="https://uat.pimeo.net/procurement/materials/suppliers/details/7/edit">https://uat.pimeo.net/procurement/materials/suppliers/details/7/edit</a> the Material Types are showing correctly. They are displayed as null, and the drop down does not populate with previously selected values</p>
          <p>Chadwicks should have Blockwork set to it, for example.</p>
        </th>
        <th>Low</th>
        <th>Company Admin</th>
      </tr>
      <tr>
        <th colspan="1">S3 bucket pimeo-test has protection on folder view, but not against viewing files if you have the direct URL. This is easy to resolve for the viewing of files within the PIMeo app, but for downloading files we would need to make some code changes (to be discussed with Sean) so that the access keys for S3 are sent with the request.</th>
        <th colspan="1">Medium (Filenames begin with a GUID so URL manipulation would be fruitless here)</th>
        <th colspan="1">Security of site</th>
      </tr>
      <tr>
        <th colspan="1">Object mappings from the database → server side code → client side code are done manually, so making changes to the database is very time consuming. If we need to add in new tables we should consider adding in a library such as AutoMapper. Combined with the below, this can affect performance of database calls.</th>
        <th colspan="1">Medium</th>
        <th colspan="1">Performance</th>
      </tr>
      <tr>
        <th colspan="1">Ties in with the above - No DTO usage so we are getting back every column from the database for every call we make. This is incredibly inefficient and if the system ends up being used heavily will need addressing.</th>
        <th colspan="1">Medium</th>
        <th colspan="1">Performance</th>
      </tr>
      <tr>
        <th colspan="1">Sorting and filtering for the grids appears to be done client-side. This can lead to the client using old data if the grid is not setup to auto-refresh. There is also usually more performance overhead with JS sorting/filtering than with a database call so generally server-side sorting is considered better.</th>
        <th colspan="1">Medium</th>
        <th colspan="1">Performance/Stale or incorrect data</th>
      </tr>
      <tr>
        <th colspan="1">In External Plant, we currently have plant and accessory split into two tables, which contain the same information. In future we should have these in one table, with a boolean to say if it is accessory or plant.</th>
        <th colspan="1">Low</th>
        <th colspan="1">Performance, will need to fix later if we stay on project</th>
      </tr>
      <tr>
        <th colspan="1">
          <figure class="image"><img src="attachments/85493777/85493778.png"></figure>
          <p>Site makes multiple requests to the server and to S3. This is causing performance issues</p>
        </th>
        <th colspan="1">Medium</th>
        <th colspan="1">Site wide</th>
      </tr>
      <tr>
        <th colspan="1">&nbsp;</th>
        <th colspan="1">&nbsp;</th>
        <th colspan="1">&nbsp;</th>
      </tr>
    </tbody>
  </table>
</figure>
<h1>Later Work&nbsp;</h1>
<p>&nbsp;</p>
<h2>Comment Flow</h2>
<p>All forms - Comments - As stated previously the flow of comments is not correct, this is further impacted by the fact there is no View of the Approved order before delivery is confirmed - we need to discuss this</p>
<p>Each area should have a comment:</p>
<ul>
  <li>when ordering the plant item, the orderer can leave a comment regarding when they need it, or further requirements.</li>
  <li>When approving or rejecting the plant item, the back office can leave a comment regarding their reason, or notes to the people on the field</li>
  <li>When confirming delivery, the receiver can leave a comment regarding condition of unit.</li>
</ul>
<p>At each step, you should be able to see the comment previously, and not overwrite it, like it happens today, so on Confirming Delivery, you should be able to see any comments left from the ordering and back office stages.</p>
<h2>Mobile Mode</h2>
<ul>
  <li>The online button shows even if you are offline or online.</li>
  <li>You cannot switch back to online automatically, you have to click the "online" button</li>
  <li>Logout is shown only on Mobile mode, not in regular mode</li>
  <li>Images on project home page do not show</li>
  <li>If you load up Test Form One, the date is missing. If you scroll (by dragging the screen) the date appears.<ul>
      <li>Clicking outside of the form box, does nothing,</li>
      <li>Clicking inside the form box does nothing</li>
      <li>Clicking in one of the disabled drop downs makes the date appear</li>
      <li>zooming in makes the date appear</li>
    </ul>
  </li>
  <li>Clicking Online while still offline will take the user to the online portion of the site, which if cached, will appear to work, but no data will be shown on anything outside of QA.</li>
</ul>
<h2>Tables around the site</h2>
<p>On bottom of each table, there is a statement of&nbsp; "Records: x of y". When you go to page 2 or 3, the number x does not increase.</p>
<p>&nbsp;</p>
<h2>Notes&nbsp;</h2>
<p>&nbsp;</p>
<p>Notes about the system, which then can be put into relevant areas later on.</p>
<h1>Development software</h1>
<ul>
  <li>Visual Studio 2017 / Visual Studio Code / JetBrains Rider</li>
  <li>ASP.NET Core</li>
  <li>Angular 6</li>
  <li>Bootstrap</li>
</ul>
<h1>Deployment</h1>
<ul>
  <li>The Development branch in the GitLab repository has been set to use GitLab CI to build the project upon every commit.&nbsp; This is then deployed via Elastic Beanstalk to a Windows EC2 instance, linked to the UAT site (we have no internal testing site for now).</li>
</ul>
<h1>Acronyms</h1>
<ul>
  <li>RFI - Request for Information</li>
  <li>CVI - Confirmation of Verbal Instruction</li>
</ul>
<p>&nbsp;</p>
<h1>Bug list</h1>
<p>Formula to get the status of each bug with a character limit</p>
<p>="["&amp;C2&amp;"]"&amp;IF(ISTEXT(D2),"["&amp;D2&amp;"]","")&amp;" - "&amp;LEFT(F2,IFERROR(FIND(" ", F2, 100),100))&amp;"…"</p>
<h1>Sending the tasks via Email</h1>
<ul>
  <li>In the subject of the email, put the task in (Column E)</li>
  <li>In the description, put in the following, and fill it out where needed (Row ID)</li>
</ul>
<p>&nbsp;</p>
<p>##status WfMT##type OriginalSpec##priority High##private off##project 79991##parent-task 164918##short-name Row ID: [Column A]Description: [column F]<br><br>Previous Comments: [Column J]</p>
<p>&nbsp;</p>
<h1>Documentation</h1>
<p>The original documents from Sean are located here:</p>
<p><a href="https://drive.google.com/drive/folders/1PmnLHX6M5chpdG_rlpj88Z5ViwUV1Rpk?usp=sharing">Google Drive</a></p>
<p>&nbsp;</p>
<p>The Plant Issue screenshots are located here:</p>
<p><a href="https://docs.google.com/presentation/d/18f8WlBbqVVSr6xYV14A7y3F7M9NNN7Dw2x_UnWjQgBQ/edit#slide=id.g3c18a958a1_0_682">Google Drive Presentation</a></p>
<h1>Document Management System (DMS)</h1>
<p>PIMeo uses the Document Management System component and API controller to create the S3 browsing modal for Plant/Material document browsing.&nbsp; It is unsuitable to use this for adding/displaying things like docket images as the file path and URL are replaced with an empty string.</p>
<p>However, there is a service behind this that can be used to for CRUD operations to S3, then the WebAPI imlementation needs to be copied from somewhere where this is already implemented (SiteWork&nbsp;→ RFI for example).</p>
<h1>Steps to update the users password or create a new password:</h1>
<p>1)&nbsp;<a href="https://asecuritysite.com/encryption/PBKDF2z">https://asecuritysite.com/encryption/PBKDF2z</a></p>
<p>2) Enter a password</p>
<p>3) Enter a 64bit salt i.e. FF02F7ED8C1812C05B021F67E2043D19 (stored in database as 0xFF02F7ED8C1812C05B021F67E2043D19)</p>
<p>4) Set Iterations to 10,000</p>
<p>5) Set Key Length to 32bit</p>
<p>6) The Hash (Base-64) value generated will need to be stored as the password for that user in the PIMeo.users.users table</p>
<p><br>&nbsp;</p>
<h1>Links to external 'screenshot' files</h1>
<p>These are files that Sean has started creating, that are more recent than the wireframes. There is also a chance he might change them too.</p>
<ul>
  <li><a href="https://docs.google.com/presentation/d/1fauk5Fd1TD6gfUQuGiemH2I3cs-XeeMbKS9z0xm8dAs/edit#slide=id.g37fc29d1be_0_183">External Plant Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1GCjlXk7nILDDpYV-p5_UaibwMbv97Vb4Ju5mUV2QqWw/edit#slide=id.p">Material Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1jhEupzHiTqHmkWYhRcMB_-pIlwhHwXQJwLRF6gZWwzc/edit?usp=sharing">Edit Projects Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/17GymiaBtGFNonGxSU0Tt2M--kc0MSUZyghNfz3nSDD8/edit#slide=id.p">Global Backend Settings Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1MxqcM7Vtxtf8IWSrmGF9AAs629cIpnQYrnXKTDKz7Yw/edit#slide=id.p">180525 Global Summary</a></li>
  <li><a href="https://docs.google.com/presentation/d/16ig8Zp-uHjR7nGPhE0z5kMosouVPHeGkPf9sW4j6LRw/edit#slide=id.p">Global Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/18f8WlBbqVVSr6xYV14A7y3F7M9NNN7Dw2x_UnWjQgBQ/edit#slide=id.g3c18a958a1_0_682">Internal Plant Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1GuDvHVmcKasSttB3eRWYZ1aOT8K54JTFdikvYXM6MYI/edit#slide=id.g3d62cee395_0_179">Dayworks Screenshots</a></li>
  <li><a href="https://drive.google.com/file/d/1mjoENKuXJyJrxZJlL_GV6lPCXb2WGBio/view">Allocation Sheet UI Revamp</a></li>
  <li><a href="https://docs.google.com/presentation/d/1AerXFaM8nEAuVSuJF6VFHI96I74d4lHV5K-n4LVWP9o/edit#slide=id.g3d77ebbefc_0_0">RFI Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1KI3bb57ziYjWi0CzqjPdLfe7tRkc3Acg_pF_gHFvRls/edit">CVI screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1tlL0YFyYTm3Yxq3B3jPZoMXjAHJrSRplL7wkIeU6cxI/edit">Submittal Screenshots</a></li>
  <li><a href="https://docs.google.com/presentation/d/1kxwRuWD1-41EViW-SZhSvVxlL9Bi6AiXonO69G_6I8M/edit">Packages Screenshots</a></li>
</ul>
<p><a href="https://docs.google.com/presentation/d/1TZWW4aCr5MbqBm99kh25vP6eHIhgKy1j3UrVnsbt0xE/edit#slide=id.g42ac0f16a6_0_1">QS Prelim Screenshots</a></p>
<p><a href="https://docs.google.com/presentation/d/1ziK1BWjTX2HnufGF-ABvon8z4-J6FWL9hl03ReUtbV0/edit#slide=id.p">QS Subcontractor Screenshots</a></p>
<p><a href="https://docs.google.com/presentation/d/133IMTfTwjF0nUY2LpqnFezVhcRkWRwnDYmHjr6JSnM8/edit#slide=id.p">QS Procurement Plant Screenshots</a></p>
<p><a href="https://docs.google.com/presentation/d/1PG9anvezjo8CeHy2kMNrBGwnnElKkErvteycKQQHsq4/edit#slide=id.p">QS Procurement Materials Screenshots</a></p>
<p><a href="https://docs.google.com/presentation/d/12GjFJRkWn9pNB4qmKR4WkQLKuYsQXnDwMC8WoihwghQ/edit#slide=id.g3f5cd7b84f_0_85">QS Projection Screenshots</a></p>
<p><a href="https://docs.google.com/presentation/d/1jVfWAKzAkcRycEL4Pxhy18Yk2kTRdMwiqhoIsRgTQ6Q/edit#slide=id.p">QS Labour Screenshots</a></p>
<p><br>&nbsp;</p>
<h1>Off Call and 'Non Off Call'</h1>
<p><br>&nbsp;</p>
<h2>Non Off Call<br>&nbsp;</h2>
<p>With Non Off Call orders, There are two options that dictate if the order is placed automatically or goes to back office for approval. These are S<strong>et Daily Spend Limit</strong>, and <strong>Daily Spend Limit is</strong><br>&nbsp;</p>
<p>Link for this page <a href="https://uat.pimeo.net/procurement/materials/admin">https://uat.pimeo.net/procurement/materials/admin</a><br>&nbsp;</p>
<figure class="image"><img src="attachments/85493780/85493781.png"></figure>
<p><br>&nbsp;</p>
<p>The daily spend limit is a cost value (determined by rate x quantity) that if the order is under, will be approved automatically.</p>
<p>The Daily Spend Limit is one of two options, setting it as per order or per day.</p>
<ul>
  <li>Per Order - The daily spend limit is the max for each order, if over, it has to go to back office for approval</li>
  <li>Per Day - The daily spend limit is the max for each day, so each day's order is accumulative</li>
</ul>
<p><br>&nbsp;</p>
<h2>Priority List&nbsp;</h2>
<p>Created by Jason Cozza on Nov 24, 2020</p>
<p>The following is a priority list created by the customer, for us to work on.</p>
<p>&nbsp;</p>
<ol>
  <li>QA/Photos offline and other issues</li>
  <li>Plant on both the Plant Dept side and the Project side</li>
  <li>Materials on both the Materials Dept side and the Project side</li>
  <li>Allocation Sheets/Timesheets (the upgrade WF is linked in the sheet along side the Allocation Sheet issues</li>
  <li>All the issues with RFI, CVI, Dayworks, Site Diary, Submittals (a bit of work with Submittals, it is not built as per the WF)</li>
  <li>Quantity Surveying - resolve the issues, try to include the Global Costs table, too</li>
  <li>Estimating - resolve the issues</li>
  <li>Everything that remains.</li>
</ol>
<p>&nbsp;</p>
<h1>Setting up the Development Environment&nbsp;</h1>
<p>&nbsp;</p>
<h2>Initial Steps</h2>
<ul>
  <li>Install NodeJS and make sure it is added to your PATH variable (restart if necessary)</li>
  <li>Sync the source code</li>
  <li>Cd to the root of the project repository</li>
  <li>run command "dotnet restore" to restore nuget packages</li>
  <li>Cd into PIMeo.Web</li>
  <li>Use npm to install webpack (currently 4.19.0 - npm install webpack@4.19.0)</li>
  <li>Run "npm i"</li>
  <li>Run "npm run build:dev"</li>
</ul>
<h2>Database Setup</h2>
<ul>
  <li>We have updated the code to use Entity Framework migrations.&nbsp; Migrations will be run when the app runs in a development environment or when it is started with the&nbsp;'--migrate true' tag on the command line<br>&nbsp;</li>
</ul>
<p>The default username\password that the system will be created with is: (Will be in BitWarden, if you don't have access ask for it over Wire)</p>
<ul>
  <li><s>There is &nbsp;SeedDemoData.sql&nbsp;script file located in PIMeo.Persistence.SQL which will fill the database with demo data.&nbsp; To use this, your database must be at the InitialCreate version.&nbsp; You can run the following command to achieve this:</s><br><s>'dotnet ef --project ../Persistence/PIMeo.Persistence.SQL -s ./ database update 20180608231453_InitialCreate'</s><br><s>After that, run the SeedDemoData file then run:</s><br><s>'dotnet ef --project ../Persistence/PIMeo.Persistence.SQL -s ./ database update' to update the database to the latest version.</s><br><br>&nbsp;</li>
</ul>
<p>Above is outdated, please contact either Chris Murrary or Jonathon Fitch for a database backup</p>
<ul>
  <li><br>&nbsp;</li>
</ul>
<h2>Using Entity Framework</h2>
<ul>
  <li>The commands for using EF are below and to be executed from the PIMeo.Web directory.<br>dotnet ef --project ../Persistence/PIMeo.Persistence.SQL -s ./ migrations add replace_me<br>dotnet ef --project ../Persistence/PIMeo.Persistence.SQL -s ./ database update</li>
</ul>
<h2>Development Notes</h2>
<ul>
  <li>Avoid using jQuery - this is an Angular Universal app that can be used with Server-Side Rendering.&nbsp; Currently this is disabled due to some issues, however, if/when we re-enable it, it will make the process faster if we don't have to replace a lot of code that causes problems when run inside a NodeJS environment.<br>You can use Angular ViewChild components to adjust element properties and classes in a SSR-safe way, or if you need to change global properties (CSS colours for example), you can create a custom component for this and add it to the main &lt;app&gt; tag.<br><br>&nbsp;</li>
  <li>Along the same line as above, any call to document/window or some other JS object that is not available in a NodeJS environment needs to be wrapped in an "if (isPlatformBrowser(PLATFORM_ID))" check.<br><br>&nbsp;</li>
  <li>Any styles added should be added to the global pimeo.css/pimeo.responsive.css files in wwwroot/css folder - due to the nature of this project we don't know if styles will be needed again so best to create global styles than component-specific styles.<br>If possible, try to create generic styles - yes this means you will need to use more classes in HTML to achieve the desire result, but it makes the needed classes much more reusable and useful.&nbsp; In pimeo.css there is a section headed by the comment /* PIMeo Generic styles */ - see this for examples.</li>
</ul>
<h2>Common Issues:</h2>
<ul>
  <li>Chrome forwarding localhost to HTTPS:<br>Go to&nbsp;<a href="chrome://net-internals/#hsts">chrome://net-internals/#hsts</a>&nbsp;and type localhost into the "Delete domain security policies" input field<br><br>&nbsp;</li>
  <li>Project hangs on the PIMEO MP4 file<br>In the ASP.Net Core logs, wait until you see:<br><br>&nbsp;</li>
</ul>
<p>PIMeo.Web&gt; webpack built 3f4a7ef1408860e43b46 in 86825ms</p>
<p><br>Then refresh the page.</p>
<ul>
  <li>Application.cs fails to move past<br>&nbsp;</li>
</ul>
<p>app.UseWebpackDevMiddleware(new WebpackDevMiddlewareOptions<br>{<br>&nbsp; &nbsp; HotModuleReplacement = true<br>});</p>
<ul>
  <li><br>This is usually due to Visual Studio failing to find NodeJS in the PATH.&nbsp; A reboot fixed this for me.<br><br>&nbsp;</li>
  <li>Dotnet restore fails with the target path to SSDT error.<br>Running "dotnet restore /p:RestoreUseSkipNonexistentTargets="false"" fixes this.<br><br>&nbsp;</li>
  <li>Cookies was not authenticated. Failure message: Unprotect ticket failed<br>This can happen when the ASP.Net Core was listening to a different port than the app was on, switching the port over to the correct one in the PIMeo.Web properties file fixed it.<br><br>&nbsp;</li>
  <li>Odd issues with ASP.NET failing to find files such as vendor.js, vendor.css<br>I don't know the specific reason for this, however, commenting out the below lines, letting the app run, then uncommenting them again appeared to work - no idea why.&nbsp; If this fails to work then try changing true to false.<br><br>app.UseWebpackDevMiddleware(new WebpackDevMiddlewareOptions<br>{<br>&nbsp; &nbsp; HotModuleReplacement = true<br>});<br><br>&nbsp;</li>
  <li>Command "npm i" can sometimes fail on windows<br>This is due to windows sometimes not releasing file locks, usually just trying the command again works, but if not the only way to get this working again is to reboot.</li>
</ul>
<p>&nbsp;</p>
<h1>CDA&nbsp;</h1>
<p>&nbsp;</p>
<p>Systems Support Project</p>
<p>Purpose:</p>
<ul>
  <li>Offer questionnaires</li>
  <li>Produce reports</li>
  <li>Primary client behind CDA is "Jaguar Land Rover" (JLR)</li>
  <li>Works in multiple languages</li>
</ul>
<p>Languages:</p>
<ul>
  <li>Handled by a CSV based lexicon<ul>
      <li>These CSVs are provided by the customer but often contain errors, the CSV uploaded should be TAB separated but they often provide COMMA separated files.</li>
      <li>Corresponding information to the identifying markers of these lexicons is controlled by&nbsp; /BBWT.Web/app/translations/translate_default.json</li>
    </ul>
  </li>
</ul>
<p>Essentially, markers exist which act almost as 'stand-ins' for the text on the site so it doesn't have to be hardcoded into the page. If no information about these markers is present in a lexicon CSV, it will default to English. These markers are easy to add to and typically semantic. ie:</p>
<p>SHARED.BUTTONS.PROCEED.TITLE</p>
<p>equal to</p>
<figure class="table">
  <table>
    <tbody>
      <tr>
        <th>Proceed</th>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li>Lexicons are uploaded in the UI per Client under Manage Clients &gt; Select Client to Edit &gt; Lexicons &gt; Upload<ul>
      <li>Lexicons can also be downloaded from this location</li>
    </ul>
  </li>
  <li>The "Clients" each contain their own lexicon, subdomain and branding<ul>
      <li>Branding determines the header logo, which if not specified would default to /Content/Images/cda-logo.png, logos should be 200x70 pixels</li>
      <li>Branding also determines colour scheme</li>
      <li>Subdomains for testing are in Route53, however the binding of subdomains is not determined by IIS, any domain can be associated with the server's Elastic IP<ul>
          <li>If no client exists, this would resolve to the default site</li>
          <li>If the client information contains the matching address used by the user, it will effectively "redirect".</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>There are 12 languages at present</li>
</ul>
<p>Reports:</p>
<ul>
  <li>Reports can be produced as Word Documents or as PDF, PDF is preferred. This is handled by Docentric.<ul>
      <li>Docentric was recently upgraded to V6.0<ul>
          <li>This included a font component overhaul allowing for non-latin text</li>
        </ul>
      </li>
      <li>Report designs are determined by a docx template with dynamic elements<ul>
          <li>CDA will not modify these dynamic elements but will often decide to modify fixed elements which can cause alignment issues on occasion</li>
        </ul>
      </li>
      <li>V6.0 seems to have a bug when dealing with JPEG elements<ul>
          <li>The current workaround is that we use PNG images exclusively, CDA are aware of this.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<p>Minor fixed-element modifications:</p>
<ul>
  <li>A small amount of inline CSS is specified in the index.cshtml file, this is a quick and dirty patch-job to sort out some styling issues.</li>
</ul>
<h1>Q8-IVR Home&nbsp;</h1>
<p>&nbsp;</p>
<h2><strong>Project Summary</strong></h2>
<h3>Problem</h3>
<p>Current existing tool, WUG (WhatsUp Gold) provides a visual overview to IDS Network operations on the real time availability of IDS sites. Currently it can only be used for IDS owned sites and some sites with direct connection (P2H) to CHost. Nevertheless, is has been proven to be a vital IDS tool to enable the timely detection and correction of sites within current scope. A major proportion (70%) of sites is not connected to WUG and form a risk of late detection and recovery when unavailable. The still difficult to be controlled sites are:</p>
<ul>
  <li>all CA sites</li>
  <li>all H2H catered sites.</li>
</ul>
<p>The lack of real time monitoring capability is a business issue:</p>
<ul>
  <li>correction only occurs after customer complaint</li>
  <li>sites may have been unavailable for days/weeks/months undetected</li>
  <li>a site problem may actual be a host or routing problem where many sites are effected without direct notice. This leads to delayed and incorrect recovery actions.</li>
</ul>
<p>The missing sites can not directly be linked to WUG. WUG is IP based and inherently needs the site IP address. This address is in many cases not available:</p>
<ul>
  <li>for security or business reasons parties do not publish their IP address</li>
  <li>site is not reachable because it is in different network.</li>
</ul>
<h3>&nbsp;Solution</h3>
<p>By introducing a virtual router in the IDS network (further named IVR), where a physical network interface is assigned multiple IP addresses that are dynamically updated by [email] messages from IDS partners, whereas:</p>
<ul>
  <li>each created virtual IP address represents one site/terminal. If that IP address is available on the virtual router, then the site/terminal is available. If the site is not available, then the IP address is dropped by the virtual router.</li>
  <li>3rd party hosts already have facilities (commonly email based) where site availability is reported to customer by automatic servers. The WUG extension then picks up these messages, parses the contents and then adds or removes the virtual IP.</li>
  <li>all P2H sites already report on application level (IFSF network management messages) every 30 to 15 minutes to CHost their availability. This information can be reported by CGI to the IDS virtual router by email report.</li>
</ul>
<p>For information, IDS uses internally to identify site terminal by 6 characters:</p>
<ul>
  <li>ISO3166 alpha 2 to specify the county code (2 characters)</li>
  <li>extended with a 4 digit IDS site code,</li>
  <li>extended with a 2 digit code to identify the terminal.</li>
</ul>
<p>To introduce the IVR into IDS, the following should be done:</p>
<ol>
  <li>Add in IDS network a virtual or physical server that caters following services:<br>
    <ul>
      <li>act as virtual router on IDS network</li>
      <li>act as mail/message server that receives site reports from specific 3rd parties</li>
      <li>interprets the 3rd part site reports to command a virtual router to set or reset corresponding IPs available.</li>
    </ul>
  </li>
  <li>Agree with 3rd parties on site message report format and service</li>
  <li>WUG to display site availability based on IP from virtual router.</li>
</ol>
<h3>Scripts&nbsp;</h3>
<p>&nbsp;</p>
<p>This project will require several different scripts to manage various tasks and responsibilities. The scripts we know we'll need so far are:<br>&nbsp;</p>
<ul>
  <li>Email Monitor - Detect that new emails have come into the server, and start the parsing script.</li>
  <li>Parsing Script - Find terminal IDs (TIDs) from a given email, and update the database and alternative IP addresses based on whether those TIDs are in the online, or offline categories of the email.<br>This script also needs to check that if a TID match is found in the database, and the state in the email is the same as in the database, that the state of the alternative IP address for this TID is correct. E.g. An email comes in with "Online: ID000001" and the parser finds a match in the database for "ID000001", but that TID is already online - the parser needs to check to make sure that IP address is 'up' (i.e. online). (May need to rethink this though, not sure how easily Python can get the state of an alternate IP address).</li>
</ul>
<p>&nbsp;</p>
<h1>Check Retention Time&nbsp;</h1>
<p>&nbsp;</p>
<p>Simple SQL for calculating when all Terminals are offline for a site:</p>
<p>&nbsp;</p>
<p>SELECT s.site_code,s.ip,s.site_status, sum(t.terminal_status)as c FROM `sites` as s</p>
<p>join terminals as t on s.site_code = t.site_code</p>
<p>group by s.site_code,s.ip,s.site_status</p>
<p>having c = 0</p>
<p>&nbsp;</p>
<h1>Email Monitor Script&nbsp;</h1>
<p><br>The email monitoring script will always be running under a daemon, watching a specific folder for new emails to come in:<br><br><i>inotifywait -m $eml_dir -e create -e moved_to |</i><br><i>&nbsp; &nbsp; &nbsp;while read path action file; do</i><br>&nbsp;</p>
<p>When a new email is detected, the script will create a log file for the current year, month, day and hour if a log does not already exist:</p>
<p><br><i>mkdir -p "$logs_dir/$cur_year/$cur_mon/$cur_day"</i><br><i>mkdir -p "$processed_dir/$cur_year/$cur_mon/$cur_day"</i></p>
<p>&nbsp;</p>
<p><br><i>if [ ! -f "$logs_dir/$cur_year/$cur_mon/$cur_day/$hour.log" ]; then</i><br><i>cat /dev/null &gt; "$logs_dir/$cur_year/$cur_mon/$cur_day/$hour.log"</i><br><i>fi</i><br>&nbsp;</p>
<p>The script logs that a new email has been detected:</p>
<p><br><i>echo "$time - New e-mail (Email_$time.eml) recieved" &gt;&gt; "$logs_dir/$cur_year/$cur_mon/$cur_day/$hour.log"</i></p>
<p><br>&nbsp;</p>
<p>and finally runs the Python parser script.<br><br><i>python ~/scripts/parser.py "$eml_dir/$file" "Email_$time.eml" "$logs_dir/$cur_year/$cur_mon/$cur_day/$hour.log" ${debug_level}</i><br><i>done</i></p>
<p>&nbsp;</p>
<h2>Email Monitor Daemon</h2>
<p>Commands:</p>
<ul>
  <li>Start service:&nbsp;<i>sudo systemctl start ivr-monitor.service</i></li>
  <li>Stop service:&nbsp;<i>sudo systemctl stop ivr-monitor.service</i></li>
  <li>Restart service:&nbsp;<i>sudo systemctl restart ivr-monitor.service</i></li>
  <li>View Logs:&nbsp;<i>sudo journalctl -u ivr-monitor.service</i></li>
  <li>Reload daemon if changes are made to the ivr-monitor.service file: <i>systemctl daemon-reload</i></li>
</ul>
<h2><br>&nbsp;Logging Levels</h2>
<p>The logging level can be passed to the email monitor script as a parameter, this needs to be specified in the <i>ivr-monitor.service</i> file, located in the directory:&nbsp;<i>/lib/systemd/system.</i></p>
<p>The following line:&nbsp;<i>ExecStart=/opt/ivr-monitor/scripts/email_monitor.sh -</i>&nbsp;can have parameters added to the end of the line, like so:<br><br><i>ExecStart=/opt/ivr-monitor/scripts/email_monitor.sh&nbsp;2&nbsp;</i></p>
<p><br>Different logging levels can be passed as a number, if no parameter is passed then the script assumes a default logging level of basic error logging.</p>
<p>The logging levels are as follows:</p>
<ul>
  <li>-1: Basic error logging, the default state</li>
  <li>0: Logging completely disabled</li>
  <li>1: "Verbose" logging</li>
  <li>2: "Verbose" logging with script start/stop/total execution time shown</li>
</ul>
<p>&nbsp;</p>
<h1>Parser&nbsp;</h1>
<p>&nbsp;</p>
<p><strong>Parser Global System Variables</strong></p>
<p>Demo Site -&nbsp;<a href="https://vrouter-dev.bbconsult.co.uk/">https://vrouter-dev.bbconsult.co.uk/</a></p>
<p>server_email = 'support@<a href="http://q8-vr.bbconsult.co.uk">q8-vr.bbconsult.co.uk</a>'<br>mailing_list = ['ivr-support@<a href="http://bbconsult.co.uk">bbconsult.co.uk</a>']<br>email_host_address = "localhost"<br>mySQLHost = "localhost"<br>mySQLUser =&nbsp;***IN LAST PASS***<br>mySQLPassword =&nbsp;***IN LAST PASS***<br>mySQLDatabase = "Q8_Database"<br>network_name = "eth0:"</p>
<p>&nbsp;</p>
<p>Live Site - 192.168.245.2 (VPN access)&nbsp;</p>
<p>server_email = 'support@<a href="http://idsvr.q8.com">idsvr.q8.com</a>'<br>mailing_list = ['ivr-support@<a href="http://bbconsult.co.uk">bbconsult.co.uk</a>']<br>email_host_address = "10.216.100.5"<br>mySQLHost = "localhost"<br>mySQLUser = ***IN LAST PASS**<br>mySQLPassword =&nbsp;***IN LAST PASS***<br>mySQLDatabase = "Q8_Database"<br>network_name = "ens160:"</p>
<p>&nbsp;</p>
<p>Breakdown of the parsing process:</p>
<ol>
  <li>A bash script (<i>monitor_emails.sh</i>) on the server detects when new email files come in to the new email folder. This script then runs the actual python parser script.</li>
  <li>The script attempts to open a file, whose file path is passed to the script as an argument. I.e. when the script is run, the command to run the script is as follows:<br><br><i>python parser.py [path to email file]</i><br><br>&nbsp;</li>
  <li>The text of the email’s body is broken down into a list of the lines, we evaluate each line to see if it's a comment line (denoted by the line beginning with '(*' and ending with '*)'), if it is, it can be ignored:<br>*****************************************************************************************************************************************************************************<br># Get the message text from the email as a string<br>body_text = eml.get_payload()<br><br># Split the message body into a list of lines, as we need to evaluate each line to check if they are a comment<br>body_lines = body_text.splitlines()<br><br>body_text = []<br><br># Lines beginning with (* and ending with *) are comments, and can be discarded<br>for i in body_lines:<br>if i[:2] != '(*' and i[-2:] == '*)':<br>&nbsp; &nbsp; &nbsp; &nbsp; body_text.append(i)*****************************************************************************************************************************************************************************</li>
  <li>The remaining lines are then recombined into a single string, special characters in the ascii value range 1 to 47 are removed, along with the semicolon character (ascii value: 59), and the whole thing is then split into a list of individual words:<br>***************************************************************************************************************# Concatenate all elements from the list into a single string<br>if type(body_text) is list:<br>&nbsp; &nbsp; body_text = ', '.join(str(v) for v in body_text)<br><br># Remove delimiters, and split the body text into individual words<br>for i in range(1, 48):<br>&nbsp; &nbsp; body_text = body_text.replace(str(unichr(i)), ' ')body_text = body_text.replace(';', ' ').split()<br>***************************************************************************************************************<br><br>&nbsp;</li>
  <li>Each word is evaluated to find either “Online:” or “Offline:” keywords, or “[2 Upper case characters][6 digit number]” terminal IDs (TIDs). If the online / offline keywords are found, any subsequent TIDs are stored in a list, corresponding to which keyword was evaluated most recently.</li>
  <li><i><strong>Pseudo Code for this process:</strong></i>
    <ol>
      <li><i>For each word in the email body:</i><br>
        <ol>
          <li><i>If the word is online: Activate the online list to receive subsequent TIDs</i></li>
          <li><i>&nbsp;&nbsp;&nbsp; If the word is offline: Activate the offline list to receive subsequent TIDs</i></li>
          <li><i>&nbsp;&nbsp;&nbsp; If the word begins with two upper case characters (ascii range 65 - 90) and is 8 characters long: add it to the currently activated list.</i></li>
          <li><i>&nbsp;<strong>* </strong>If the word begins with two upper case characters&nbsp;(ascii range 65 - 90)&nbsp;and is not 8 characters long: send an error notification.</i></li>
          <li><i>&nbsp;&nbsp;&nbsp; If the word does not match any of the above: ignore it.</i><br><i>* Planned, not yet implemented.</i><br><i>&nbsp;</i></li>
        </ol>
      </li>
    </ol>
  </li>
  <li>The script then connects to the MySQL database, and updates the relevant rows on the “Status Table”, to reflect any terminal status changes made by the email:<br><br><i><strong>My Code:</strong></i><br>cur.execute("UPDATE Status_table SET Status=\"Online\", Last_Changed=" +<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; datetime.datetime.now() + " WHERE TID=" + on[2:])OR<br>cur.execute("UPDATE Status_table SET Status=\"Offline\", Last_Changed=" +<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; datetime.datetime.now() + " WHERE TID=" + on[2:])<br><br><i><strong>Explanation:</strong></i><br>The above lines of code execute an SQL statement on the MySQL database, where the “status” (online or offline) of a terminal, specified by the TID (terminal ID), is updated to the correct value (depending on which “keyword” preceded it most recently).</li>
  <li>Finally, the python script makes a system call, which tells Ubuntu to set an IP address to being “up”, or “down” depending on whether the associated TID is online or offline respectively. If an IP address is “up” that IP address can be locally “pinged”, it cannot be “pinged” however, if that IP address is “down. The system call from Python looks like this:<br><br>call(["ifconfig", "eth0:" + eth0_addr, ip, "netmask", "255.255.255.0", "up"]) to set an IP address as "up"<br>ORcall(["ifconfig", "eth0:" + eth0_addr, ip, "netmask", "255.255.255.0", "down"]) to set an IP address as "down"</li>
</ol>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3><strong>Debugging</strong></h3>
<p>Various levels of debugging can be enabled via passing an argument to the email monitoring Bash script. By default, if no argument is passed, debugging will assume that errors should be logged and support emails should be sent when errors are detected. The following codes are arguments which can be passed to enable different debugging levels than the standard - only log errors - debugging level:</p>
<ul>
  <li>0 : Completely disable logging</li>
  <li>1 : Enable comprehensive logging (i.e. When emails were received, when email parsing starts, when it finishes, if all IP addresses had matches found in the database, if the terminal statuses were updated successfully)</li>
  <li>2 : Comprehensive logging, with additional logs to measure how long certain parts of the script take to ru</li>
</ul>
<p>&nbsp;</p>
<h2>Parser - Specific email format tweaks&nbsp;</h2>
<p>&nbsp;</p>
<h3>Sterling Petromani Emails</h3>
<p>Emails from Sterling Petromani do not have the Content-Transfer-Encoding specified in the header of the email, therefore the parser must look in the email body for this:<br><br>&nbsp;</p>
<p>if "Content-Transfer-Encoding: base64" in body_text:<br>&nbsp; &nbsp; &nbsp; &nbsp; body_text = base64.decodestring(body_text.split("Content-Transfer-Encoding: base64", 1)[-1])<br><br>&nbsp;</p>
<p>in order to correctly decode it, the parser must remove the part of the email body, before and up to the "Content-Transfer-Encoding" line.</p>
<p>&nbsp;</p>
<h2>Unit Testing&nbsp;</h2>
<p>&nbsp;</p>
<p>There are two different scripts used for unit testing - "unit_test.py" and "bulk_unit_test.py" on the virtual router, they should always be located in the "/home/ubuntu/scripts/" directory. These unit tests ensure that the system performs all of it's necessary functions, i.e. updating the database to reflect the status of terminals given in emails, putting relevant IP addresses online or offline and creating system logs; they also ensure that errors are identified correctly. Any time a change is made to the "parser.py" script, these unit test files should be run to ensure that previous functionality was not affected by the changes. Both unit test scripts send emails, which contain reserved IP addresses, so that these tests do not interfere with any real TIDs; generally speaking, the "unit_test.py" script sends emails with only a few terminal ID numbers, in order to test the core functionality of the system; whereas many of the tests in the "bulk_unit_test.py" script use between 10 and 20 TIDs in their testing emails.&nbsp;<br><br>The process for enabling the unit tests is as follows:</p>
<ol>
  <li>Ensure that the "Unit Test Code" region of the "parser.py" script is uncommented, i.e. at the top and bottom of the code block, there are three single quotation marks - these need to be removed in order to allow the unit test scripts to function correctly.</li>
  <li>Ensure that the logging level of the Email Monitor Script is set to 1 or 2. The details of how to change the logging level can be found here:&nbsp;<a href="Email-Monitor-Script_85493788.html">Email Monitor Script</a>.</li>
</ol>
<p>&nbsp;</p>
<p>The unit tests can be run with the command "sudo python unit_test.py&nbsp;[unit test type]" or&nbsp;"sudo python bulk_unit_test.py&nbsp;[unit test type]" where "unit test type" is a number from 0 to 2, indicating the formatting of the emails that the unit tests send.&nbsp;</p>
<p>The email formats available are:</p>
<p>0 - Plain Text<br>1 - HTML<br>2 - Rich Text</p>
<p>So in order to run the "unit_test.py" tests where the emails are formatted as HTML you would run the command as:<br><br><i>&nbsp; &nbsp; sudo python unit_test.py 1</i></p>
<p>or if you wanted to run the "bulk_unit_test.py" tests where the emails are formatted as Rich Text, you would run the command as:<br><br><i>&nbsp; &nbsp; sudo python bulk_unit_test.py 2</i></p>
<p>&nbsp;</p>
<p>If no number is specified when running the script, it will default to sending the emails as plain text.</p>
<p>&nbsp;</p>
<h2>Updating the Database&nbsp;</h2>
<p>&nbsp;</p>
<p>There is a simple script to update the database using a CSV file in the "/home/ubuntu/scripts/" directory, called "update_database.py". This can be run with the following command:</p>
<p>"<i>sudo python update_database.py&nbsp;[name of your csv file]</i>"&nbsp;e.g. "sudo python update_database.py vrouter_table.csv".<br><br>The script will display an error message if it is run without a CSV file specified as an argument.</p>
<p>&nbsp;</p>
<h2>Uploading New File Versions&nbsp;</h2>
<p>&nbsp;</p>
<p>In order to upload new files to the Q8 live server, you'll need to sign into the Q8 network and then connect to the server via PuTTY (<a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html">Latest PuTTY release</a>) and PSFTP (Included in the PuTTY installer). Connecting via PuTTY is straightforward as it has a GUI which should look like:</p>
<figure class="image"><img src="attachments/85493794/85493798.png"></figure>
<p>The up-to-date IP address and port number are in LastPass under the Q8 IVR SSH note in the BB-Shared-Customers\Q8 folder. Once the details have been entered, click "Open" to connect to the server, you'll be prompted to enter the username and password into the terminal which are also found in the Q8 IVR SSH note. Once you've logged you can use the terminal to interact with the server's shell (I'll come back to this).</p>
<p>Then log into the server with PSFTP, do this by opening a Command Prompt in Windows, and then typing "PSFTP [IP Address]&nbsp;-P [Port Number]" so it should look like this:&nbsp;</p>
<figure class="image"><img src="attachments/85493794/85493795.png"></figure>
<p>Again, you'll be prompted for a username and password, which are the same as in the Q8 IVR SSH note. Once you've logged in, you can upload files from your local PC to the server using the "put [Filename]" command, you can navigate to different directories on your local computer using the "lcd&nbsp;[Directory name]" command and you can navigate to different directories on the server using the standard "cd" command.</p>
<p>On the live server, I recommend uploading the files you're changing to the home directory, and then moving them from there to where they need to be, in order to avoid permission issues when uploading via PSFTP. The parser.py file needs to be moved from the home directory to the "/opt/ivr-monitor/scripts/" directory once it's been ftp'd over; assuming the daemon service is still running, all you should need to do is move the file over and it should just work as normal.</p>
<p>&nbsp;</p>
<p>Finally, for the following files, you'll need to change the values of the database password inside the scripts (You can edit the files on the server using "sudo nano [filename]") in order to ensure that they work on the Live server, as the versions on SVN use the database password for the test server:</p>
<ul>
  <li>Server Web Files/index.php</li>
  <li>Python Scripts/bulk_unit_test.py</li>
  <li>Python Scripts/parser.py</li>
  <li>Python Scripts/unit_test.py</li>
  <li>Python Scripts/update_database.py</li>
</ul>
<p>The database password can be found in the Q8 IVR SSH note in LastPass.</p>
<p>&nbsp;</p>
<p>Uploading New File Versions&nbsp;</p>
<p>&nbsp;</p>
<p>In order to upload new files to the Q8 live server, you'll need to sign into the Q8 network and then connect to the server via PuTTY (<a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html">Latest PuTTY release</a>) and PSFTP (Included in the PuTTY installer). Connecting via PuTTY is straightforward as it has a GUI which should look like:</p>
<figure class="image"><img src="attachments/85493794/85493798.png"></figure>
<p>The up-to-date IP address and port number are in LastPass under the Q8 IVR SSH note in the BB-Shared-Customers\Q8 folder. Once the details have been entered, click "Open" to connect to the server, you'll be prompted to enter the username and password into the terminal which are also found in the Q8 IVR SSH note. Once you've logged you can use the terminal to interact with the server's shell (I'll come back to this).</p>
<p>Then log into the server with PSFTP, do this by opening a Command Prompt in Windows, and then typing "PSFTP [IP Address]&nbsp;-P [Port Number]" so it should look like this:&nbsp;</p>
<figure class="image"><img src="attachments/85493794/85493795.png"></figure>
<p>Again, you'll be prompted for a username and password, which are the same as in the Q8 IVR SSH note. Once you've logged in, you can upload files from your local PC to the server using the "put [Filename]" command, you can navigate to different directories on your local computer using the "lcd&nbsp;[Directory name]" command and you can navigate to different directories on the server using the standard "cd" command.</p>
<p>On the live server, I recommend uploading the files you're changing to the home directory, and then moving them from there to where they need to be, in order to avoid permission issues when uploading via PSFTP. The parser.py file needs to be moved from the home directory to the "/opt/ivr-monitor/scripts/" directory once it's been ftp'd over; assuming the daemon service is still running, all you should need to do is move the file over and it should just work as normal.</p>
<p>&nbsp;</p>
<p>Finally, for the following files, you'll need to change the values of the database password inside the scripts (You can edit the files on the server using "sudo nano [filename]") in order to ensure that they work on the Live server, as the versions on SVN use the database password for the test server:</p>
<ul>
  <li>Server Web Files/index.php</li>
  <li>Python Scripts/bulk_unit_test.py</li>
  <li>Python Scripts/parser.py</li>
  <li>Python Scripts/unit_test.py</li>
  <li>Python Scripts/update_database.py</li>
</ul>
<p>The database password can be found in the Q8 IVR SSH note in LastPass.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h2>Websites Account Details&nbsp;</h2>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>Test:</p>
<figure class="table">
  <table>
    <tbody>
      <tr>
        <th>Page</th>
        <th>URL</th>
        <th colspan="1">Bit Warden</th>
      </tr>
      <tr>
        <th>Main Page</th>
        <th><a href="https://vrouter-dev.bbconsult.co.uk/">https://vrouter-dev.bbconsult.co.uk/</a></th>
        <th colspan="1">Q8 Site Password TEST</th>
      </tr>
      <tr>
        <th>Database - MyPhpAdmin</th>
        <th><a href="https://vrouter-dev.bbconsult.co.uk/bbadmin">https://vrouter-dev.bbconsult.co.uk/bbadmin</a></th>
        <th colspan="1">Q8 BB Admin TEST</th>
      </tr>
      <tr>
        <th>Email Server - Roundcube</th>
        <th><a href="https://vrouter-dev.bbconsult.co.uk/roundcude/">https://vrouter-dev.bbconsult.co.uk/roundcude/</a></th>
        <th colspan="1">Q8 RoundCube Email TEST</th>
      </tr>
    </tbody>
  </table>
</figure>
<p>&nbsp;</p>
<p>Live:</p>
<p>&nbsp;</p>
<h1>Oobeedoo&nbsp;</h1>
<p>&nbsp;</p>
<h2>GitLab:</h2>
<p><a href="https://gitlab.bbconsult.co.uk/blueberry/oobeedoo/-/tree/master">https://gitlab.bbconsult.co.uk/blueberry/oobeedoo/-/tree/master</a></p>
<h2>Device Hardware</h2>
<ul>
  <li>PI 3a+</li>
  <li>32gb micro SD card</li>
  <li>Speaker (driver)</li>
  <li>Adafruit MAX98357 I2S Class-D Mono Amp</li>
  <li>PTM button</li>
</ul>
<p>&nbsp;</p>
<h2>Setup Instructions</h2>
<p>Use case – initial setup</p>
<ol>
  <li>Create an account on the Oobeedoo mobile application</li>
  <li>Plug the device in</li>
  <li>Power device up</li>
  <li>Wait for audio instructions</li>
  <li>If no user found<ol>
      <li>Hold button for &gt; 5 &lt; 10 seconds</li>
      <li>Wait for audio instructions</li>
      <li>Connect to Oobeedoo Wi-Fi from laptop or phone<ol>
          <li>Sign in to Oobeedoo account</li>
          <li>Sign in to Premium Spotify* account</li>
          <li>Sign in to local Wi-Fi</li>
          <li>Wait for audio instructions (from device)</li>
        </ol>
      </li>
      <li>Go to Oobeedoo webpage (<a href="https://dashboard.oobeedoo.co.uk">https://dashboard.oobeedoo.co.uk</a>)<ol>
          <li>Sign in</li>
          <li>Navigate to ‘Devices’ page (left hand side of screen)</li>
          <li>Click ‘Manage’ under Oobeedoo</li>
          <li>Navigate to ‘Music Providers’ page (left hand side of screen)</li>
          <li>Click Connect under Spotify</li>
          <li>Wait for audio instructions (from device)</li>
        </ol>
      </li>
      <li>Device will reboot</li>
    </ol>
  </li>
  <li>Device logged in</li>
  <li>Press button for &lt; 5 seconds (normal click) to play music</li>
  <li>Music will need to be added to default playlist on the webpage to work</li>
  <li>Device will play music<br><br>&nbsp;</li>
</ol>
<h2>BUG NOTICE:</h2>
<p>If device reboots after step 5.e. and indicates through audio that no user has been found a second reboot by holding the button for &gt; 10 seconds will often fix this issue.</p>
<p>If you hold the button for under 10 seconds it will remove the user settings and a setup will have to take place again.</p>
<h2>Use case – setting up with a different account</h2>
<p>Set up is the same as above but at step 5 when a user is found go to step 5.a.</p>
<h2>Use case – day to day use after setup</h2>
<p>If the device has been fully setup, day to day use will be powering up the device, device will log in to users account and Spotify and a click of the button will play/stop the music that has been pre-set in the default playlist on the oobeedoo dashboard.</p>
<p>If the device cannot connect to the WIFI it will alert the user through and audio warning.</p>
<p>&nbsp;</p>
<h2>Button Controls</h2>
<p>Single button press – will play/stop music if logged in if not will give audio instructions to sign in.</p>
<p>Button hold for 5 seconds – Device will go into setup mode, step (5.a.).</p>
<p>Button hold for more than 10 seconds – Device will soft reboot.</p>
<p>&nbsp;</p>
<h2>Code and Issues</h2>
<h3>Main Current Issue:</h3>
<p>Once a device has been connected and gone through the setup process it will not retain the user credentials. Although this is somewhat not true as the device will often still connect to the local Wi-Fi correctly,</p>
<p>therefore the device is holding at least the Wi-Fi information but the program is not recognising this on reboot (step 5.e. + 6.).</p>
<p>On occasion this can be subverted by holding the device button for more than 10 seconds calling a soft reboot (sudo shutdown -r now). Once the device has rebooted it will then recognise the account and sign in.</p>
<h3>Possible Causes:</h3>
<ol>
  <li>The device on boot could be passing the code of signing in before the device has had time to connect to the internet therefore not being able to verify the account it will assert no details recognised.</li>
  <li>A previous bug caused the resin Wi-Fi connect (step 5.c.) to never close, this meant that after a user signed in the instructions and next process (step 5.c.iv) would never happen.&nbsp;<br>It was concluded from looking at the PI’s running processes that this was holding the device in a lock therefore killing the process after the details has been taken fixed this. But this may have caused this to be unstable.&nbsp;<br>Tests at the developers WFH location on 3 different Wi-Fi networks passed, yet this may be causing the issue for the speaker manufacture. Could be killing the process before the details have been stored.<br>Location on device ‘/oobeedoo/src/oobeedoo_player.py’. Code Lines ~258 – 262.<br>&nbsp;</li>
</ol>
<figure class="image"><img src="attachments/85493802/85493803.png"></figure>
<p>3. User details are wrong and the code only asserts details not found through audio instructions.</p>
<h2>Connection</h2>
<p>Device will display its IP address on screen upon boot.</p>
<p>From Terminal:</p>
<p>SSH pi@IPAddress</p>
<p>&nbsp;</p>
<h2>Passwords</h2>
<p>The main Password for the device is contained within the company Bit-Warden.</p>
<p>&nbsp;</p>
<h2>Things to note</h2>
<p><i>Display</i> - Pi may not display on older screens due to the HDMI configs, plugging into a tv often works.</p>
<p><i>Volume</i> – Volume of the device is not constant/consistent, although this is set during the setup in the main python code to be 80% and the firebase values are set to around this it reverts to a much lower volume inconsitantly.</p>
<p><i>Audio</i> – Audio often contains artefacts on start-up.</p>
<p><i>IMG</i> – IMG for the software is bloated, it sits at 32GB but this is mainly empty space it can be taken down to around 4GB by deleting the empty space.</p>
<p><i>Website</i> – The website will only allow one playlist for music, will also create an additional device every time the oobeedoo is set up on the account it will not replace duplicate devices.</p>
<p>Therefore when on each time setting the device up you will have to find the newest device and connect that to the Spotify.</p>
<p>Note: Change the name of each device you add to a number so you can see the new device which will always be named Oobeedoo.&nbsp;</p>
<p>&nbsp;</p>
