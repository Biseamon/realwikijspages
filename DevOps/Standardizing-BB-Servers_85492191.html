<!DOCTYPE html>
<html>
    <head>
        <title>Blueberry : Standardizing BB Servers</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">Blueberry</a></span>
                            </li>
                                                    <li>
                                <span><a href="DevOps-Information_36144018.html">DevOps Information</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Blueberry : Standardizing BB Servers
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                        
        
    
        
    
        
        
            Created by <span class='author'> Martin Green</span> on Jul 21, 2020
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <h1 id="StandardizingBBServers-OverallGoals">Overall Goals</h1><p>We need to standardize our processes for working with servers to<br/><br/></p><ul style="list-style-type: square;"><li>Reduce the time spent creating and maintaining servers<ul style="list-style-type: square;"><li>In particular, doing small fixes like disk space updates</li><li>By making server configurations self-documenting<br/><br/></li></ul></li><li>Save AWS costs<ul style="list-style-type: square;"><li>By using CMM / Switching to Serverless</li><li>By not using Windows so much</li><li>By saving disk space<br/><br/></li></ul></li><li>Improve security and reliability<ul style="list-style-type: square;"><li>By limiting who has access</li><li>By reducing manual changes to servers</li></ul></li></ul><h1 id="StandardizingBBServers-ServerTypes">Server Types</h1><p>Blueberry has</p><ul style="list-style-type: square;"><li>Core servers - used to run the company.  Mostly Linux, but not entirely.  Mostly need to be 24-7<br/>but not all.  Managed by Sysadmins, not PMs.<br/><br/></li><li>Test servers - used to test customer projects.  Often Linux at Consult, mostly Windows at BB <br/>Systems.  Mostly have embedded database on the VM, but not always.<br/><br/></li><li>Live servers - used to run customer systems.  Consult generally pushes these to customer accounts<br/>but not completely.  Systems has more Live systems, and these are often expensive.  But if we're<br/>hosting, we're usually being paid to host.</li></ul><p><br/></p><p>Maintenance of core servers is a completely different issue to test and live servers.</p><h1 id="StandardizingBBServers-LegacyvsFutureServers">Legacy vs Future Servers</h1><p>We have ~190 legacy servers.  The truth is that making significant changes to these servers is hard to<br/>justify economically - because any manual intervention costs hours of engineer time, and because<br/>for customer servers we need to get customer permission.</p><p>However, there are still some actions we may be able to take.  There many be a reasonable number<br/>of simpler servers which could be converted to a new approach.</p><h1 id="StandardizingBBServers-ServerHosting&amp;ManagementSystems">Server Hosting &amp; Management Systems</h1><p>Technology is racing forward, and there are many new ways of deploying and managing software<br/>in the cloud. In particular, we see the following technologies as relevant<br/><br/></p><p><strong style="letter-spacing: 0.0px;">Standard Servers</strong></p><p>The current approach - we have standard EC2 servers which run software - which generally we<br/>manually install.<br/><br/></p><p><strong style="letter-spacing: 0.0px;">Docker - Containerisation </strong></p><p>Docker provides a lightweight alternative to virtual machines.  An application can be packaged with<br/>it's dependencies into a container image, which can then be run on various different types of<br/>container hosts.</p><p>One major advertised benefit of Docker is that it provides a clean interface between the application<br/>developers and sysadmins - the developer packages the application into a docker container, which<br/>is delivered to sysadmins to run.  In practice, I'm not sure how often this really happens.</p><p>Docker has significant limitations compared to VM technology</p><ul style="list-style-type: square;"><li>It has no process isolation - a bad docker container could attack nearby ones.  Not a<br/>huge concern for us</li><li>Practically speaking, it's mostly a Linux solution. Docker for Windows exists, but the image files <br/>are bigger and it requires Windows hosts.</li><li>I think here is no RDP GUI - you can't get to the desktop.  But this is something we're trying<br/>to wean ourselves off.</li></ul><p>But the big issue with Docker from an application hosting perspective is that the systems and solutions<br/>for hosting containers are complex, require specialist skills and are relatively expensive.</p><p>Docker containers can just be run on a server using standard docker technology.  But more often they<br/>are run using an &quot;orchestration system&quot; which allows many containers to be run across multiple host<br/>servers.</p><p>There are multiple orchestration systems</p><ul style="list-style-type: square;"><li>Amazon has Elastic Container Service and Fargate.   The former requires more hands-on<br/>management, the later is a service to run a single container at a time.</li><li>Google has Cloud Run which just runs a container and you pay while it's working</li><li>Kubernetes is a very large system from Google which is supported by many clouds - <br/>it allows multiple containers to be run across servers, and has a huge suite of tools - it's<br/>famously complex.  Entreprises like it because in theory they can redeploy their whole<br/>cluster on a different cloud quickly.  Kubernetes effectively offers an alternative to learning<br/>AWS tools - instead you learn K8.</li></ul><p>I have investigated these hosting systems, and in general I found that they are quite a bit more<br/>expensive than standard EC2 servers.  There are lots of gotchas - Google Cloud Run looks<br/>interesting, but it's not clear that the &quot;turn off when unused&quot; would really work.</p><p>I am quite apprehensive about Kubernetes - it looks like a huge time sink.</p><p>There is also the risk that we pick one system, find issues, switch to another - and then the<br/>AWS comes out with a new system and we have to move again.</p><p>Having said this, using Docker is a <em style="letter-spacing: 0.0px;">significant </em>benefit over running multiple applications on<br/>a single server.</p><p>I can see a situation where we convert many of our core servers to docker containers and<br/>run them on some AWS Container platform.  But it raises questions</p><ul style="list-style-type: square;"><li>Which system would we run the containers on?  Standard Docker, ECS, K8?</li><li>How much extra might it cost, if anything?</li><li>Are we sure we can manage performance and other issues?</li></ul><p><br/><strong><strong>Serverless</strong></strong></p><p>Serverless is a new approach which has no server at all. It refers to both a general<br/>technology and a specific tool:  the Serverless Framework <a class="external-link" href="https://www.serverless.com/" rel="nofollow">https://www.serverless.com/</a></p><p>In this approach, we build SPA web-apps which use a REST API to deliver data to the client.  <br/>The code which handles REST calls is put into Lambda functions, which AWS executes for us<br/>and bills us for.  AWS is completely responsible for making the server resources available<br/>and scaling to meet need.</p><p>Although the CPU cost of running Lambda functions is low, it's actually 3x more expensive<br/>than EC2 - except that with EC2 we rarely max out the server for long periods.  Serverless<br/>is usually a fraction of traditional web-server hosting cost - because most web applications<br/>are used very intermittently.</p><p>Serverless is only really a solution for new BBWT3 projects, because it needs significant<br/>changes in the code.  It's generally not an option for core servers.</p><p>Serverless has potentially a huge impact on future system hosting</p><ul style="list-style-type: square;"><li>It should deliver a huge cost saving, with automatic and free scalability with<br/>zero management overhead</li><li>It eliminates CMM, which whilst a useful solution has been a huge time-sync</li><li>Sites would be available 24-7 with no interruptions</li><li>It forces BB teams to design systems to be stateless, which is a good thing</li></ul><p><br/>Converting BBWT3 to run Serverless will require some changes, but is currently expected to <br/>be a multiple-week problem, not a multiple-month problem.  For example, we need to <br/>remove Hangfire, the background scheduler.</p><p>Also, by definition, Serverless applications require the database to be external to the <br/>application - whereas current practice with small BBWT3 systems is to host the database on <br/>the same server in many cases.   Giving each test system it's own RDS database would eliminate <br/>most of the cost savings, so we should instead look at having a common shared DB server used <br/>by multiple Serverless applications - we need to investigate this further.</p><p>Adoption of Serverless would require changes to PTS, because PTS currently tracks &quot;servers&quot;<br/>extracted from AWS - and these would not exist.</p><p>Widespread adoption of BBWT3 Serverless for new BB web applications would be a huge<br/>step change in our server management - and potentially very attractive to customers.  It<br/>would achieve many of the goals listed at the top of this document in one step.</p><p>What about Legacy servers?  We'd want to look at BBWT3 systems, and see which - if any -<br/>might be economically converted to this new approach.  Then we'd probably want to<br/>consider the legacy and new systems separately, but we probably would need to treat<br/>the legacy systems as something to &quot;manage out&quot;.<br/><br/></p><p><strong>Standard Servers + Ansible</strong></p><p>Ansible is a server configuration technology which converts server setup into scripts.</p><p>Instead of manually logging on to a server, and installing SQL Express, we tell Ansible<br/>about the server, then tell it to run a &quot;playbook&quot; for installing SQL Express on the server</p><p>Ansible is a mature and powerful technology which is very well supported and understood -<br/>it can be used to manage 100s of servers. </p><p>It also works with both Windows and Linux servers - although it is mostly a Linux tool<br/>and must be run on Linux.</p><p>However, it's not immediately clear exactly how Ansible would be used within our setup.</p><p>My current understanding</p><ul style="list-style-type: square;"><li>We have a directory of servers, which we pull from AWS, probably including AWS tags</li><li>We tag all servers with descriptive information &quot;BBWT3 server&quot;, &quot;BBWT2&quot; etc</li><li>Ansible can query the servers to record facts - &quot;Windows 2019&quot;, &quot;Ubuntu 18.4&quot; &quot;20gb free disk space&quot;</li><li>We then create a suite of scripts to manage the servers</li></ul><p><br/>My concerns / questions</p><ul style="list-style-type: square;"><li>Ansible could get quite complex - if we have multiple tags on the servers, and each<br/>tag triggers different scripts.  This could be efficient, but will need careful design and<br/>management</li><li>We've not defined e.g. how Ansible solves problems like &quot;out of disk space&quot; on a server<br/>or implementing Windows Security patches.  It needs more time to understand</li><li>We should look at AWX - the Ansible GUI for server management</li></ul><p><span style="letter-spacing: 0.0px;"><br/>Ansible can be used to deliver significant standardization benefits.  The goal should be <br/>to create Ansible scripts that can recreate a server from scratch - all the details of<br/>how to setup the server are defined in scripts.  </span>In this scenario, if the server has no local <br/>state - i.e. no local files - then we may not need to backup the server at all:  if it dies, we <br/>just re-run the Ansible script to create a new version of the system.</p><p>We currently have a situation where we'd like to move all systems to the new Amazon<br/>T3 instances because they offer better value.  Sysadmins have needed to do this<br/>manually, taking a lot of time.  If we had an Ansible script for each server, we'd just<br/>create new servers and re-run the scripts.<br/><br/></p><p>Ansible can potentially be an alternative to Docker:  if we have a Linux server where we <br/>want to run multiple applications we can:</p><ul style="list-style-type: square;"><li>Manually install multiple applications on the server (as now)</li><li>Install docker, and install containerised versions of the applications on the server</li><li>Write an Ansible script which specifies the full list of software to install</li></ul><p><br/></p><p>Docker purists will say that the Ansible option is not as good, but I'm not sure there<br/>are real practical benefits to Docker in this situation?</p><p>Question:  if we remove some application from the Ansible Playbook, will Ansible<br/>uninstall it?  Not clear.</p><p><br/>Ansible can still potentially be useful with Serverless hosting, because we still have a database<br/>that may need maintenance, and there are integrations with other systems to manage.</p><p>The big question is how to organise Ansible.  Do we have one script per server, or .. something<br/>that drives the setup based on tags?</p><p><br/></p><p><strong>NixOS + PowerShell DSC</strong></p><p>NixOS is a new Linux Distribution that uses the Nix Package Manager.  <a class="external-link" href="https://nixos.org/nixos/manual/" rel="nofollow">https://nixos.org/nixos/manual/</a></p><p>PowerShell DSC is similar technology for Windows - <a class="external-link" href="https://docs.microsoft.com/en-us/powershell/scripting/dsc/overview/overview?view=powershell-7" rel="nofollow">https://docs.microsoft.com/en-us/powershell/scripting/dsc/overview/overview?view=powershell-7</a></p><p><br/></p><p>As far as I can tell, Ansible is a scripting system, not a declarative system.   NixOS and DSC<br/>are both fully declarative.</p><p><br/>A declarative configuration management system does not have a script which specifies<br/>steps to perform.  Instead, it has a declaration of the desired state.</p><p>We don't say &quot;install SQL Express&quot;.  We say &quot;make sure the server has SQL Express installed&quot;</p><p>The difference is that if we remove the &quot;SQL Express&quot; item from the list, and re-run the<br/>tool, it will <em>uninstall</em> SQL Express.</p><p><br/></p><p>NixOS and PowerShell DSC are potentially significantly better than Ansible - because they <br/>reduce the need to figure out what to do.</p><p>For example, we could have scripts that specify the disk size for each server - if we need to <br/>increase the disk space, we just change the value in the script and the tools handle the change.</p><p><br/>BUT - these technologies are both fairly new, and significantly less understood than Ansible.</p><p>Note:  PowerShell PSC can be run from Ansible, but this isn't quite the point...</p><p><br/>I suspect these tools are probably too big a jump for us.</p><p><br/><br/></p><h1 id="StandardizingBBServers-Conclusions">Conclusions</h1><p>I see these conclusions</p><p><br/><strong>New Web Systems - Serverless</strong></p><p><span style="letter-spacing: 0.0px;">We should investigate moving BBWT3 to Serverless ASAP as the benefits are strong.  If<br/>this proves possible, we should try to make it our standard platform.</span></p><p><span style="letter-spacing: 0.0px;">If it's not possible, we'll need to rethink <img class="emoticon emoticon-smile" src="images/icons/emoticons/smile.png" data-emoticon-name="smile" alt="(smile)"/></span></p><p><span style="letter-spacing: 0.0px;"><br/></span></p><p><span style="letter-spacing: 0.0px;"><strong>New Web Systems - PM Goals</strong></span></p><p><span style="letter-spacing: 0.0px;">Regardless of Serverless, we should be asking all PMs to move towards stateless<br/>web servers:</span></p><ol><li>Ask PMs to always use S3 for files</li><li>Move away from local databases - create a &quot;test&quot; DB server</li><li>Move to Linux + MySQL by default</li></ol><p><br/>Note that #1, #2 are required for Serverless.  #3 becomes irrelevant if we do Serverless.</p><p><br/></p><p><strong style="letter-spacing: 0.0px;">Core Servers</strong></p><p><span style="letter-spacing: 0.0px;">My current thinking is that we're probably better trying to use Ansible exclusively.  I'm<br/>not against Docker, and it might be useful - but if we have Ansible scripts for each<br/>server, I'm not sure Docker has enough benefits?</span></p><p><span style="letter-spacing: 0.0px;">However, we need to understand the right way of using Ansible across many servers?</span></p><p><span style="letter-spacing: 0.0px;">Note - if sysadmins think there is a big benefit in moving many systems to docker,<br/>I'll consider it further.</span></p><p><span style="letter-spacing: 0.0px;"><br/></span></p><p><span><strong>Legacy Servers</strong></span></p><p>This is a much more complex issue.</p><p>If Serverless works, then we should look at all BBWT3 test systems and see if any<br/>might be moved to Serverless. I suspect it's unlikely.</p><p>The next step is to try to create Ansible scripts for Legacy Systems where possible<br/>and then test that recreating the system with Ansible works.  Ideally, we'd try to<br/>make the server stateless, although this won't be possible that often.</p><p>We should look hard at two technologies that might assist us</p><ul style="list-style-type: square;"><li>Using EFS to give servers some spare disk space without increasing the actual disk</li><li>Mounting S3 as a drive to move storage off the server without changing code</li></ul><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p>
                    </div>

                    
                 
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on Dec 03, 2020 14:51</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
